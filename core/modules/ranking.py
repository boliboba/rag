import torch
import gc
import signal
import time
from contextlib import contextmanager
from FlagEmbedding import FlagLLMReranker

from core.config import RERANKER_MODEL, USE_RERANKER, RERANKER_TOP_K
from core.utils.singletons import lazy_singleton

@contextmanager
def gpu_memory_manager():
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è GPU –ø–∞–º—è—Ç—å—é"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        yield
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def timeout_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–∞–π–º–∞—É—Ç–∞"""
    raise TimeoutError("–†–µ—Ä–∞–Ω–∫–µ—Ä –ø—Ä–µ–≤—ã—Å–∏–ª –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")

def rerank_with_timeout(reranker, pairs, timeout_seconds=30):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–≥–Ω–∞–ª –¥–ª—è —Ç–∞–π–º–∞—É—Ç–∞ (—Ç–æ–ª—å–∫–æ –Ω–∞ Unix —Å–∏—Å—Ç–µ–º–∞—Ö)
    try:
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
        
        start_time = time.time()
        scores = reranker.compute_score(pairs)
        elapsed = time.time() - start_time
        
        signal.alarm(0)  # –û—Ç–∫–ª—é—á–∞–µ–º —Ç–∞–π–º–∞—É—Ç
        signal.signal(signal.SIGALRM, old_handler)  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        
        print(f"‚è±Ô∏è –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–Ω—è–ª–æ {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
        return scores
        
    except (AttributeError, OSError):
        # Windows –∏–ª–∏ —Å–∏—Å—Ç–µ–º–∞ –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ - –ø—Ä–æ—Å—Ç–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        start_time = time.time()
        scores = reranker.compute_score(pairs)
        elapsed = time.time() - start_time
        print(f"‚è±Ô∏è –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–Ω—è–ª–æ {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
        return scores

@lazy_singleton
def get_reranker():
    if not USE_RERANKER:
        return None
    
    try:
        with gpu_memory_manager():
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU 1 –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            device = 'cpu'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é CPU
            gpu_device_id = None
            
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                print(f"üîç –î–æ—Å—Ç—É–ø–Ω–æ GPU: {gpu_count}")
                
                if gpu_count > 1:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ç–æ—Ä—É—é –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
                    device = 'cuda:1'
                    gpu_device_id = 1
                    print(f"üöÄ –†–µ—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ GPU 1")
                else:
                    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
                    device = 'cuda:0'
                    gpu_device_id = 0
                    print(f"‚ö†Ô∏è –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ GPU –¥–æ—Å—Ç—É–ø–Ω–∞, —Ä–µ—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ GPU 0")
            
            use_fp16 = torch.cuda.is_available()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π GPU
            if gpu_device_id is not None:
                torch.cuda.set_per_process_memory_fraction(0.7, device=gpu_device_id)
            
            reranker = FlagLLMReranker(
                RERANKER_MODEL, 
                use_fp16=use_fp16,
                device=device
            )
            
            print(f"‚úÖ –†–µ—Ä–∞–Ω–∫–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {device}")
            return reranker
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞: {e}")
        return None

def rerank_documents(query, docs, reranker=None, top_k=None):
    if not docs:
        return docs
    
    if reranker is None:
        reranker = get_reranker()
        
    if reranker is None:
        return docs[:top_k] if top_k else docs
    
    if top_k is None:
        top_k = RERANKER_TOP_K
    
    try:
        with gpu_memory_manager():
            print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            max_content_length = 512  # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            pairs = []
            for i, doc in enumerate(docs):
                content = doc.page_content[:max_content_length]
                pairs.append([query, content])
                if i < 3:  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    print(f"  –î–æ–∫—É–º–µ–Ω—Ç {i+1}: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(pairs)} –ø–∞—Ä –Ω–∞ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ...")
            scores = rerank_with_timeout(reranker, pairs, timeout_seconds=60)
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã —Å–∫–æ—Ä—ã –¥–ª—è {len(scores)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            scored_docs = list(zip(docs, scores))
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            reranked_docs = [doc for doc, _ in scored_docs[:top_k]]
            print(f"üìä –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: –æ—Ç–æ–±—Ä–∞–Ω–æ {len(reranked_docs)} –∏–∑ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return reranked_docs
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return docs[:top_k] if top_k else docs

def cleanup_reranker():
    """–û—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞"""
    get_reranker.reset()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()