import torch
import gc
from contextlib import contextmanager
from FlagEmbedding import FlagLLMReranker

from core.config import RERANKER_MODEL, USE_RERANKER, RERANKER_TOP_K
from core.utils.singletons import lazy_singleton

@contextmanager
def gpu_memory_manager():
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è GPU –ø–∞–º—è—Ç—å—é"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    yield
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()



@lazy_singleton
def get_reranker():
    if not USE_RERANKER:
        return None
    
    with gpu_memory_manager():
        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU 1 –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        device = 'cpu'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é CPU
        gpu_device_id = None
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"üîç –î–æ—Å—Ç—É–ø–Ω–æ GPU: {gpu_count}")
            
            if gpu_count > 1:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ç–æ—Ä—É—é –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
                device = 'cuda:1'
                gpu_device_id = 1
                print(f"üöÄ –†–µ—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ GPU 1")
            else:
                # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
                device = 'cuda:0'
                gpu_device_id = 0
                print(f"‚ö†Ô∏è –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ GPU –¥–æ—Å—Ç—É–ø–Ω–∞, —Ä–µ—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ GPU 0")
        
        use_fp16 = torch.cuda.is_available()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π GPU
        if gpu_device_id is not None:
            torch.cuda.set_per_process_memory_fraction(0.7, device=gpu_device_id)
        
        reranker = FlagLLMReranker(
            RERANKER_MODEL, 
            use_fp16=use_fp16,
            device=device
        )
        
        print(f"‚úÖ –†–µ—Ä–∞–Ω–∫–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {device}")
        return reranker

def rerank_documents(query, docs, reranker=None, top_k=None):
    if not docs:
        return docs
    
    if reranker is None:
        reranker = get_reranker()
        
    if reranker is None:
        return docs[:top_k] if top_k else docs
    
    if top_k is None:
        top_k = RERANKER_TOP_K
    
    with gpu_memory_manager():
        print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        max_content_length = 512  # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
        pairs = []
        for i, doc in enumerate(docs):
            content = doc.page_content[:max_content_length]
            pairs.append([query, content])
            if i < 3:  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                print(f"  –î–æ–∫—É–º–µ–Ω—Ç {i+1}: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(pairs)} –ø–∞—Ä –Ω–∞ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        scores = reranker.compute_score(pairs)
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã —Å–∫–æ—Ä—ã –¥–ª—è {len(scores)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        scored_docs = list(zip(docs, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        reranked_docs = [doc for doc, _ in scored_docs[:top_k]]
        print(f"üìä –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: –æ—Ç–æ–±—Ä–∞–Ω–æ {len(reranked_docs)} –∏–∑ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return reranked_docs

def cleanup_reranker():
    """–û—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞"""
    get_reranker.reset()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()