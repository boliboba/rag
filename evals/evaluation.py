import json
import os
import pandas as pd
import numpy as np
import torch
import asyncio
import gc
from contextlib import contextmanager
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from bleurt_pytorch import BleurtForSequenceClassification, BleurtTokenizer

# –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å TPU
try:
    import torch_xla
    import torch_xla.core.xla_model as xm
    HAS_TPU = True
    print("üöÄ TPU –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
except ImportError:
    HAS_TPU = False
    print("‚ö†Ô∏è TPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ CPU/GPU")

from core.config import MODEL_NAME
from core.llm.deepeval_adapter import OpenRouterDeepEvalAdapter
from core.llm.chains import split_docs
from core.db import get_embedding_model
from core.utils.singletons import lazy_singleton

from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRelevancyMetric,
    GEval
)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
def get_device():
    if HAS_TPU:
        return xm.xla_device()
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

@contextmanager
def gpu_memory_manager():
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è GPU/TPU –ø–∞–º—è—Ç—å—é"""
    if HAS_TPU:
        # –û—á–∏—Å—Ç–∫–∞ TPU –ø–∞–º—è—Ç–∏
        xm.mark_step()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    yield
    
    if HAS_TPU:
        # –û—á–∏—Å—Ç–∫–∞ TPU –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –æ–ø–µ—Ä–∞—Ü–∏–π
        xm.mark_step()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    gc.collect()

@lazy_singleton
def get_bleurt_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å BLEURT –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    with gpu_memory_manager():
        device = get_device()
        device_type = "TPU" if HAS_TPU else ("GPU" if torch.cuda.is_available() else "CPU")
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º BLEURT –º–æ–¥–µ–ª—å –Ω–∞ {device_type}")
        
        model = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20')
        model.eval()
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        model = model.to(device)
            
        return model

@lazy_singleton
def get_bleurt_tokenizer():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏ BLEURT"""
    return BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20')

def calculate_bleurt_score(references, candidates):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç BLEURT –æ—Ü–µ–Ω–∫—É –º–µ–∂–¥—É —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏"""
    model = get_bleurt_model()
    tokenizer = get_bleurt_tokenizer()
    
    if model is None or tokenizer is None:
        print("‚ö†Ô∏è BLEURT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏")
        return [0.0] * len(references)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–º–µ–Ω—è–µ–º –∏—Ö
    processed_candidates = []
    for i, candidate in enumerate(candidates):
        if candidate is None or candidate == "":
            processed_candidates.append("–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è BLEURT –æ—Ü–µ–Ω–∫–∏ #{i}, –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞'")
        else:
            processed_candidates.append(candidate)
    
    with gpu_memory_manager():
        with torch.no_grad():
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            device = next(model.parameters()).device
            
            # Batch processing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            batch_size = 8
            all_scores = []
            
            for i in range(0, len(references), batch_size):
                batch_refs = references[i:i+batch_size]
                batch_cands = processed_candidates[i:i+batch_size]
                
                inputs = tokenizer(batch_refs, batch_cands, padding='longest', return_tensors='pt', truncation=True, max_length=512)
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã–≤–æ–¥
                outputs = model(**inputs)
                
                # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è TPU, –Ω—É–∂–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å
                if HAS_TPU:
                    batch_scores = xm.mesh_reduce('bleurt_scores', outputs.logits.flatten(), lambda x: x)
                    batch_scores = batch_scores.cpu().tolist()
                else:
                    batch_scores = outputs.logits.flatten().cpu().tolist()
                
                all_scores.extend(batch_scores)
                
                # –û—á–∏—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –ø–∞–º—è—Ç—å
                del inputs, outputs
                if HAS_TPU:
                    xm.mark_step()
                elif torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            return all_scores

def calculate_cosine_similarity(texts1, texts2):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —ç–º–±–µ–¥–¥–µ—Ä –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    embedding_model = get_embedding_model()
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–æ–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤
    embeddings1 = []
    embeddings2 = []
    similarities = []
    
    for i, (text1, text2) in enumerate(zip(texts1, texts2)):
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if text2 is None or text2 == "":
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ #{i}, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0.0")
                similarities.append(0.0)
                continue
                
            emb1 = embedding_model.embed_query(text1)
            emb2 = embedding_model.embed_query(text2)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
            similarity = cosine_similarity([emb1], [emb2])[0][0]
            similarities.append(similarity)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ #{i}: {e}")
            similarities.append(0.0)
    
    return similarities


def create_test_cases(dataset, system_responses=None, limit=None):
    if limit is not None and limit < len(dataset):
        if system_responses is not None:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –≤—ã–±–æ—Ä–∫–µ
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π random_state –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)
            
            # –ï—Å–ª–∏ system_responses –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∏–∑–≤–Ω–µ, –≤–∞–∂–Ω–æ —Å–±—Ä–æ—Å–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã
            # —á—Ç–æ–±—ã –æ–Ω–∏ —Å–æ–≤–ø–∞–¥–∞–ª–∏ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            system_responses = system_responses.reset_index(drop=True)
            
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if len(dataset) != len(system_responses):
                min_len = min(len(dataset), len(system_responses))
                dataset = dataset.iloc[:min_len]
                system_responses = system_responses.iloc[:min_len]
        else:
            dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)
    
    test_cases = []
    for i, (_, row) in enumerate(tqdm(dataset.iterrows(), total=len(dataset), desc="–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤")):
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è actual_output
        if system_responses is not None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤–º–µ—Å—Ç–æ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ
            actual_output = system_responses.iloc[i]["system_answer"] if "system_answer" in system_responses.columns else system_responses.iloc[i]["answer"]
        else:
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–≤–µ—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –¥–ª—è actual_output, –∏ –¥–ª—è expected_output
            actual_output = row["answer"]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ª—É—á–∞–π
        test_case = LLMTestCase(
            input=row["question"],
            actual_output=actual_output,
            expected_output=row["answer"],
            retrieval_context=split_docs(row["context"]) if isinstance(row["context"], str) else row["context"]
        )
        test_cases.append(test_case)
    
    return test_cases

def evaluate_dataset(dataset, system_responses=None, model_name=MODEL_NAME, temperature=0.0, limit=None):
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
    test_cases = create_test_cases(dataset, system_responses, limit)
    eval_model = OpenRouterDeepEvalAdapter(model_name=model_name, temperature=temperature)
    
    # RAG Triad –∏ Correctness
    metrics = [
        FaithfulnessMetric(threshold=0.5, model=eval_model),
        AnswerRelevancyMetric(threshold=0.5, model=eval_model),
        ContextualRelevancyMetric(threshold=0.5, model=eval_model),
        GEval(
            name="Correctness",
            criteria="–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ '—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥' –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –Ω–∞ –æ—Å–Ω–æ–≤–µ '–æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—ã–≤–æ–¥–∞'.",
            evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
            threshold=0.5,
            model=eval_model
        )
    ]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    references = [test_case.expected_output for test_case in test_cases]
    candidates = [test_case.actual_output for test_case in test_cases]
    
    # –†–∞—Å—á–µ—Ç BLEURT –æ—Ü–µ–Ω–æ–∫
    bleurt_scores = calculate_bleurt_score(references, candidates)
    
    # –†–∞—Å—á–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
    cosine_scores = calculate_cosine_similarity(references, candidates)
    
    results = []
    for i, test_case in enumerate(tqdm(test_cases, desc="–û—Ü–µ–Ω–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤")):
        metric_scores = {}
        for metric in metrics:
            try:
                metric.measure(test_case)
                score = metric.score    
            except Exception as e:
                score = None
            metric_name = metric.__class__.__name__.replace("Metric", "")
            metric_scores[metric_name] = score

        # –î–æ–±–∞–≤–ª—è–µ–º BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –æ—Ü–µ–Ω–∫–∏
        metric_scores["BLEURT"] = bleurt_scores[i]
        metric_scores["CosineSimilarity"] = cosine_scores[i]

        # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if system_responses is not None:
            result_item = {
                "question": test_case.input,
                "system_answer": test_case.actual_output,
                "golden_answer": test_case.expected_output,
                **metric_scores,
                "avg_score": np.mean(list(metric_scores.values())),
            }
        else:
            result_item = {
                **dataset.iloc[i].to_dict(),
                **metric_scores,
                "avg_score": np.mean(list(metric_scores.values())),
            }
            
        results.append(result_item)
    
    return pd.DataFrame(results)

def generate_report(evaluated_df):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–Ω–µ –≤—Ö–æ–¥—è—â–∏–µ –≤ –∏—Å–∫–ª—é—á–∞–µ–º—ã–π —Å–ø–∏—Å–æ–∫)
    exclude_cols = ["question", "answer", "system_answer", "golden_answer", 
                   "context", "chunk_ids", "avg_score"]
    metric_columns = [col for col in evaluated_df.columns if col not in exclude_cols]
    
    stats = {}
    # –î–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    for metric in metric_columns:
        scores = evaluated_df[metric].dropna().tolist()
        if scores:
            stats[metric] = {
                "mean": float(np.mean(scores)),
                "min": float(np.min(scores)),
                "max": float(np.max(scores)),
                "median": float(np.median(scores))
            }
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–µ
    avg_scores = evaluated_df["avg_score"].dropna()
    if len(avg_scores) > 0:
        stats["avg_score"] = {
            "mean": float(np.mean(avg_scores)),
            "min": float(np.min(avg_scores)),
            "max": float(np.max(avg_scores)),
            "median": float(np.median(avg_scores))
        }
    else:
        stats["avg_score"] = {"mean": 0.0, "min": 0.0, "max": 0.0, "median": 0.0}
    
    return stats

def filter_best_examples(evaluated_df, threshold=0.5):
    best_df = evaluated_df[evaluated_df["avg_score"] >= threshold].copy()
    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(best_df)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {len(evaluated_df)} (–ø–æ—Ä–æ–≥: {threshold})")
    return best_df

def save_results(dataset, output_path, report=None):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    dataset.to_csv(output_path, index=False)
    
    if report:
        report_path = os.path.join(
            os.path.dirname(output_path), 
            f"{os.path.splitext(os.path.basename(output_path))[0]}_report.json"
        )
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

def stop():
    """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –≤—Å–µ —Å–∏–Ω–≥–ª—Ç–æ–Ω—ã –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"""
    get_bleurt_model.reset()
    get_bleurt_tokenizer.reset()
    
    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if HAS_TPU:
        xm.mark_step()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    gc.collect()

async def calculate_bleurt_score_async(references, candidates):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç BLEURT –æ—Ü–µ–Ω–∫—É –º–µ–∂–¥—É —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏"""
    # –ü–æ—Å–∫–æ–ª—å–∫—É BLEURT —Ä–∞–±–æ—Ç–∞–µ—Ç —Å GPU –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —É–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã,
    # –ø—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    return calculate_bleurt_score(references, candidates)

async def calculate_cosine_similarity_async(texts1, texts2):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤"""
    # –ü–æ—Å–∫–æ–ª—å–∫—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ - –∑–∞—Ç—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è,
    # –ø—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    return calculate_cosine_similarity(texts1, texts2)

async def evaluate_dataset_async(dataset, system_responses=None, model_name=MODEL_NAME, temperature=0.0, limit=None, max_concurrency=6):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ evaluate_dataset, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è a_measure –¥–ª—è –º–µ—Ç—Ä–∏–∫"""
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
    test_cases = create_test_cases(dataset, system_responses, limit)
    eval_model = OpenRouterDeepEvalAdapter(model_name=model_name, temperature=temperature)
    
    # RAG Triad –∏ Correctness
    metrics = [
        FaithfulnessMetric(threshold=0.5, model=eval_model),
        AnswerRelevancyMetric(threshold=0.5, model=eval_model),
        ContextualRelevancyMetric(threshold=0.5, model=eval_model),
        GEval(
            name="Correctness",
            criteria="–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ '—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥' –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –Ω–∞ –æ—Å–Ω–æ–≤–µ '–æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—ã–≤–æ–¥–∞'.",
            evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
            threshold=0.5,
            model=eval_model
        )
    ]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    references = [test_case.expected_output for test_case in test_cases]
    candidates = [test_case.actual_output for test_case in test_cases]
    
    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
    bleurt_task = asyncio.create_task(calculate_bleurt_score_async(references, candidates))
    cosine_task = asyncio.create_task(calculate_cosine_similarity_async(references, candidates))
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM
    semaphore = asyncio.Semaphore(max_concurrency)
    
    async def process_test_case(i, test_case):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ª—É—á–∞–π —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        async with semaphore:
            metric_scores = {}
            
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            measure_tasks = []
            for metric in metrics:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ a_measure
                if hasattr(metric, 'a_measure'):
                    async def measure_metric(metric, test_case):
                        try:
                            return await metric.a_measure(test_case)
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–µ—Ç—Ä–∏–∫–∏ {metric.__class__.__name__}: {e}")
                            return None
                    task = asyncio.create_task(measure_metric(metric, test_case))
                else:
                    # –ï—Å–ª–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–µ—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤ –¥—Ä—É–≥–æ–º –ø–æ—Ç–æ–∫–µ
                    loop = asyncio.get_event_loop()
                    task = loop.run_in_executor(None, lambda: metric.measure(test_case))
                measure_tasks.append((metric, task))
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
            for metric, task in measure_tasks:
                await task
                metric_name = metric.__class__.__name__.replace("Metric", "")
                metric_scores[metric_name] = metric.score
            
            return i, metric_scores
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    tasks = [process_test_case(i, test_case) for i, test_case in enumerate(test_cases)]
    metric_results = await asyncio.gather(*tasks)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥—Ä—É–≥–∏—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
    bleurt_scores = await bleurt_task
    cosine_scores = await cosine_task
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    for i, metric_scores in sorted(metric_results):
        test_case = test_cases[i]
        
        # –î–æ–±–∞–≤–ª—è–µ–º BLEURT –∏ –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –æ—Ü–µ–Ω–∫–∏
        metric_scores["BLEURT"] = bleurt_scores[i]
        metric_scores["CosineSimilarity"] = cosine_scores[i]
        
        # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if system_responses is not None:
            result_item = {
                "question": test_case.input,
                "system_answer": test_case.actual_output,
                "golden_answer": test_case.expected_output,
                **metric_scores,
                "avg_score": np.mean(list(metric_scores.values())),
            }
        else:
            result_item = {
                **dataset.iloc[i].to_dict(),
                **metric_scores,
                "avg_score": np.mean(list(metric_scores.values())),
            }
            
        results.append(result_item)
    
    return pd.DataFrame(results) 