#!/usr/bin/env python
import sys
import os
# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, os.path.abspath(os.path.dirname(os.path.dirname(__file__))))

import asyncio
import json
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å TPU
try:
    import torch_xla
    import torch_xla.core.xla_model as xm
    import torch_xla.distributed.parallel_loader as pl
    import torch_xla.distributed.xla_multiprocessing as xmp
    HAS_TPU = True
    print("üöÄ TPU –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω")
except ImportError:
    HAS_TPU = False
    print("‚ö†Ô∏è TPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ CPU/GPU")

from core.llm import get_llm
from core.db import get_vector_store
from core.llm.chains import get_retrieval_chain, format_docs, retrieve, rerank
from core.config import MODEL_NAME as DEFAULT_MODEL_NAME, PROMPTS

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from evals.evaluation import (
    evaluate_dataset,
    evaluate_dataset_async,
    generate_report,
    save_results,
    stop as stop_evaluation
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("evaluate_models.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ - —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
MODELS_TO_EVALUATE = [
    "google/gemini-2.5-flash-preview-05-20",
    "qwen/qwen3-235b-a22b", 
    "qwen/qwen3-32b", 
    "qwen/qwen3-14b",
    "google/gemma-3-27b-it",
    "google/gemma-3-12b-it",
    "meta-llama/llama-3.1-405b-instruct",
    "meta-llama/llama-3.3-70b-instruct",
    "meta-llama/llama-4-maverick",
    "meta-llama/llama-4-scout"
]

# –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
TEMPERATURE = 0.0

# –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ–º–æ–π –º–æ–¥–µ–ª–∏)
EVAL_MODEL_NAME = "google/gemini-2.5-flash-preview-05-20"

# –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
TEST_DATASET_PATH = "data/filtered_evaluated_dataset.csv"

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (None –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞)
LIMIT = 200

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π
MAX_CONCURRENCY = 6

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –ø—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
_document_cache = {}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
def get_device():
    if HAS_TPU:
        return xm.xla_device()
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

def precompute_documents_for_all_questions(dataset, limit=None):
    """–ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑"""
    if limit is not None and limit < len(dataset):
        dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)
    
    global _document_cache
    _document_cache.clear()
    
    print(f"üîç –ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è {len(dataset)} –≤–æ–ø—Ä–æ—Å–æ–≤...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = get_device()
    device_type = "TPU" if HAS_TPU else ("GPU" if torch.cuda.is_available() else "CPU")
    print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {device_type} –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π")
    
    for idx, (_, row) in enumerate(tqdm(dataset.iterrows(), total=len(dataset), desc="–ü—Ä–µ–¥–ø–æ—Å—á—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")):
        question = row["question"]
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        docs = retrieve(question)
        print(f"–ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(docs)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
        from core.config import USE_RERANKER
        if USE_RERANKER:
            # –í–ö–õ–Æ–ß–ê–ï–ú –†–ï–†–ê–ù–ö–ï–† –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            reranked_docs = rerank(question, docs)
            print(f"–†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(reranked_docs)}")
            final_docs = reranked_docs
        else:
            # –†–µ—Ä–∞–Ω–∫–µ—Ä –æ—Ç–∫–ª—é—á–µ–Ω - –±–µ—Ä—ë–º —Ç–æ–ø-5 –±–µ–∑ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            final_docs = docs[:5] if len(docs) > 5 else docs
            print(f"–†–µ—Ä–∞–Ω–∫–µ—Ä –æ—Ç–∫–ª—é—á–µ–Ω - –≤–∑—è—Ç–æ {len(final_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        formatted_context = format_docs(final_docs)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        _document_cache[question] = formatted_context
    
    print(f"‚úÖ –ü—Ä–µ–¥–ø–æ—Å—á—ë—Ç –∑–∞–≤–µ—Ä—à—ë–Ω! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(_document_cache)} –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤")
    return dataset

def create_model_specific_chain(model_name):
    """–°–æ–∑–¥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ü–µ–ø–æ—á–∫—É –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    prompt = ChatPromptTemplate.from_template(PROMPTS["qa"])
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LLM –¥–ª—è —ç—Ç–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
    llm = get_llm(model_name=model_name, temperature=TEMPERATURE)
    
    def get_cached_context(query):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫—ç—à–∞"""
        global _document_cache
        return _document_cache.get(query, "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    rag_chain = (
        {"context": get_cached_context, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

async def generate_system_responses_async(dataset, model_name, limit=None, max_concurrent_questions=6):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–æ–ø—Ä–æ—Å–æ–≤"""
    if limit is not None and limit < len(dataset):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ random_state=42 –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å create_test_cases
        dataset = dataset.sample(limit, random_state=42).reset_index(drop=True)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ü–µ–ø–æ—á–∫—É –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
    retrieval_chain = create_model_specific_chain(model_name)
    
    # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
    semaphore = asyncio.Semaphore(max_concurrent_questions)
    
    async def process_question(question, golden_answer, question_idx):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å"""
        async with semaphore:
            print(f"\n[{model_name}] –í–æ–ø—Ä–æ—Å {question_idx + 1}: {question}")
            # –î–æ–±–∞–≤–ª—è–µ–º /no_think –¥–ª—è –º–æ–¥–µ–ª–µ–π Qwen —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º thinking
            modified_question = question
            if "qwen" in model_name.lower():
                modified_question = f"{question} /no_think"
                print(f"[{model_name}] –î–æ–±–∞–≤–ª–µ–Ω —Ñ–ª–∞–≥ /no_think –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞ thinking")
            
            result = await retrieval_chain.ainvoke(modified_question)
            answer = result
            print(f"[{model_name}] –û—Ç–≤–µ—Ç {question_idx + 1}: {answer[:100]}..." if len(answer) > 100 else f"[{model_name}] –û—Ç–≤–µ—Ç {question_idx + 1}: {answer}")
            
            return {
                "question": question,
                "system_answer": answer,
                "golden_answer": golden_answer
            }
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    tasks = []
    for idx, (_, row) in enumerate(dataset.iterrows()):
        task = process_question(row["question"], row["answer"], idx)
        tasks.append(task)
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é {len(tasks)} –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è {model_name} (–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {max_concurrent_questions})")
    responses = await asyncio.gather(*tasks)
    
    return pd.DataFrame(responses)

async def evaluate_model(model_name, dataset, output_dir, limit=None):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å"""
    # –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ª–æ–≥–æ–≤
    model_dir_name = model_name.replace('/', '_')
    model_dir = output_dir / model_dir_name
    model_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏: {model_name}")
    start_time = time.time()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã
    system_responses = await generate_system_responses_async(
        dataset=dataset,
        model_name=model_name,
        limit=limit,
        max_concurrent_questions=6  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 6 –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –º–æ–¥–µ–ª—å
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
    system_responses_path = model_dir / "system_responses.csv"
    system_responses.to_csv(system_responses_path, index=False)
    logger.info(f"–û—Ç–≤–µ—Ç—ã —Å–∏—Å—Ç–µ–º—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {system_responses_path}")
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º EVAL_MODEL_NAME –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {EVAL_MODEL_NAME} –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ {model_name}")
    evaluation_df = await evaluate_dataset_async(
        dataset=dataset,
        system_responses=system_responses,
        model_name=EVAL_MODEL_NAME,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –∏ —Ç—É –∂–µ –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        temperature=0.6,
        limit=limit,
        max_concurrency=6  # –õ–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM –¥–ª—è –º–µ—Ç—Ä–∏–∫
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
    raw_evaluation_path = model_dir / "raw_evaluation.csv"
    evaluation_df.to_csv(raw_evaluation_path, index=False)
    logger.info(f"–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {raw_evaluation_path}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = generate_report(evaluation_df)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—Ç—á–µ—Ç
    results_path = model_dir / "evaluation_results.csv"
    save_results(evaluation_df, str(results_path), report)
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã
    stop_evaluation()
    
    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    import gc
    gc.collect()
    
    if HAS_TPU:
        # –û—á–∏—Å—Ç–∫–∞ TPU –ø–∞–º—è—Ç–∏
        xm.mark_step()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    elapsed_time = time.time() - start_time
    logger.info(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
    
    return {
        "model": model_name,
        "metrics": report,
        "elapsed_time": elapsed_time,
        "examples_count": len(dataset) if limit is None else min(len(dataset), limit)
    }

async def run_evaluations(models, dataset, output_dir, limit, concurrency):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞"""
    semaphore = asyncio.Semaphore(concurrency)
    
    async def run_with_semaphore(model):
        async with semaphore:
            return await evaluate_model(
                model_name=model,
                dataset=dataset,
                output_dir=output_dir,
                limit=limit
            )
    
    tasks = [run_with_semaphore(model) for model in models]
    return await asyncio.gather(*tasks)

async def main():
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("./results") / f"eval_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –ª–æ–≥–æ–≤
    device_type = "TPU" if HAS_TPU else ("GPU" if torch.cuda.is_available() else "CPU")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞
    with open(output_dir / "config.json", "w") as f:
        json.dump({
            "models": MODELS_TO_EVALUATE,
            "eval_model": EVAL_MODEL_NAME,
            "temperature": TEMPERATURE,
            "dataset": TEST_DATASET_PATH,
            "limit": LIMIT,
            "device": device_type,
            "optimization": "precomputed_docs_tpu_optimized" if HAS_TPU else "precomputed_docs_gpu_optimized"
        }, f, indent=2)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = pd.read_csv(TEST_DATASET_PATH)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å {len(dataset)} –ø—Ä–∏–º–µ—Ä–∞–º–∏")
    
    # üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–µ–¥–ø–æ—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    logger.info("üîç –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–µ–¥–ø–æ—Å—á—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤...")
    dataset = precompute_documents_for_all_questions(dataset, limit=LIMIT)
    
    logger.info(f"üöÄ –ë—É–¥–µ—Ç –æ—Ü–µ–Ω–µ–Ω–æ {len(MODELS_TO_EVALUATE)} –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–µ–¥–ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    results = await run_evaluations(
        models=MODELS_TO_EVALUATE,
        dataset=dataset,
        output_dir=output_dir,
        limit=LIMIT,
        concurrency=MAX_CONCURRENCY
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    summary_path = output_dir / "summary.json"
    with open(summary_path, "w") as f:
        json.dump(results, f, indent=2)
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    summary_data = []
    for result in results:
        row = {
            "model": result["model"],
            "examples_count": result["examples_count"],
            "elapsed_time": result["elapsed_time"]
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        for metric_name, metric_values in result.get("metrics", {}).items():
            row[f"{metric_name}_mean"] = metric_values.get("mean", np.nan)
        
        summary_data.append(row)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(output_dir / "summary_table.csv", index=False)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
    global _document_cache
    _document_cache.clear()
    
    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    from core.modules.ranking import cleanup_reranker
    cleanup_reranker()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    stop_evaluation()
    import gc
    gc.collect()
    
    if HAS_TPU:
        # –û—á–∏—Å—Ç–∫–∞ TPU –ø–∞–º—è—Ç–∏
        xm.mark_step()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    logger.info(f"üéâ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(MODELS_TO_EVALUATE)} –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –¥–ª—è {device_type}")

if __name__ == "__main__":
    # –ò–∑–±–µ–≥–∞–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(main()) 